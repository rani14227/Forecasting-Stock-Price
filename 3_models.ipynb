{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26ff17db-75c7-4e7e-bffc-af6a2695e220",
   "metadata": {},
   "source": [
    "# ARIMA MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab3557-edda-485f-8788-470acb225534",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb269c9-ce36-4100-a6f3-67e42edd5b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import logging\n",
    "import plotly.io as pio\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, HTML\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname=s - %(message=s')\n",
    "\n",
    "def download_stock_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading stock data for ticker: {ticker}\")\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        data.reset_index(inplace=True)\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data.set_index('Date', inplace=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download stock data: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses the stock data by adding technical indicators.\"\"\"\n",
    "    logging.info(\"Starting preprocessing of data\")\n",
    "    \n",
    "    # Adding technical indicators without normalizing\n",
    "    data['SMA_10'] = talib.SMA(data['Close'], timeperiod=10)\n",
    "    data['EMA_10'] = talib.EMA(data['Close'], timeperiod=10)\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    data['MACD'], data['MACD_signal'], _ = talib.MACD(data['Close'])\n",
    "    data.fillna(method='bfill', inplace=True)\n",
    "    logging.info(\"Data preprocessing complete\")\n",
    "    return data\n",
    "\n",
    "def main() -> pd.DataFrame:\n",
    "    ticker_symbol = 'TATAELXSI.NS'\n",
    "    start_date = '2019-04-01'\n",
    "    end_date = '2024-03-31'\n",
    "    stock_data = download_stock_data(ticker_symbol, start_date, end_date)\n",
    "    preprocessed_data = preprocess_data(stock_data)\n",
    "    return preprocessed_data\n",
    "\n",
    "# Run main and get the preprocessed data\n",
    "preprocessed_data = main()\n",
    "\n",
    "# Displaying the first 5 rows with a title\n",
    "display(HTML('<h2>First 5 Rows of the DataFrame</h2>'))\n",
    "display(HTML(preprocessed_data.head(5).to_html()))\n",
    "# Displaying the last 5 rows with a title\n",
    "display(HTML('<h2>Last 5 Rows of the DataFrame</h2>'))\n",
    "display(HTML(preprocessed_data.tail(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651dc9f-6203-4fb1-8dc2-8cf5b9a631ef",
   "metadata": {},
   "source": [
    "### Data aggregation\n",
    "\n",
    "#### convert daily data into monthly data with some aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d15746-9d11-4857-b630-d4dce644513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_to_monthly_average(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aggregates daily data into monthly averages.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting the aggregation of data to monthly averages.\")\n",
    "        monthly_data_avg = data.resample('M').mean()\n",
    "        logging.info(\"Data successfully aggregated to monthly averages.\")\n",
    "        return monthly_data_avg\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to aggregate data: {e}\")\n",
    "        raise\n",
    "\n",
    "def plot_trend_comparison(daily_data: pd.DataFrame, monthly_data: pd.DataFrame, title=\"Daily vs. Monthly Trend Comparison\") -> None:\n",
    "    \"\"\"Plots the daily trend and monthly average trend side-by-side using Plotly.\"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting to plot the trend comparison.\")\n",
    "        fig = make_subplots(rows=1, cols=2, subplot_titles=('Daily Trend', 'Monthly Average Trend'))\n",
    "        \n",
    "        # Daily plot\n",
    "        daily_trace = go.Scatter(x=daily_data.index, y=daily_data['Close'], mode='lines', name='Daily Close', line=dict(color='blue'))\n",
    "        fig.add_trace(daily_trace, row=1, col=1)\n",
    "        \n",
    "        # Monthly plot\n",
    "        monthly_trace = go.Scatter(x=monthly_data.index, y=monthly_data['Close'], mode='lines', name='Monthly Close', line=dict(color='red'))\n",
    "        fig.add_trace(monthly_trace, row=1, col=2)\n",
    "        \n",
    "        # Layout\n",
    "        fig.update_layout(title_text=title, hovermode='x', showlegend=False)\n",
    "        \n",
    "        pio.show(fig)  # Display the plot inline\n",
    "        logging.info(\"Successfully plotted the trend comparison.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to plot the trend comparison: {e}\")\n",
    "        raise\n",
    "\n",
    "# Aggregate and plot the data\n",
    "monthly_data = aggregate_to_monthly_average(preprocessed_data)\n",
    "plot_trend_comparison(preprocessed_data, monthly_data)\n",
    "\n",
    "\n",
    "#print(monthly_data.head(5))\n",
    "# Displaying the last 5 rows with a title\n",
    "display(HTML('<h2>Last 5 Rows of the DataFrame</h2>'))\n",
    "display(HTML(monthly_data.tail(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ed244-527c-4e90-babb-e3a06ded483e",
   "metadata": {},
   "source": [
    "### convert to stationary to implement model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b547a-f6c2-42a0-978d-7a73adef527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "def test_stationarity(timeseries: pd.Series) -> None:\n",
    "    \"\"\"Tests and logs the stationarity of the provided timeseries.\"\"\"\n",
    "    logging.info(\"Testing the stationarity of the timeseries\")\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    logging.info(\"Results of Dickey-Fuller Test:\")\n",
    "    logging.info(dfoutput)\n",
    "\n",
    "def make_stationary(data: pd.DataFrame, target_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Transforms the target column to make the data stationary.\"\"\"\n",
    "    logging.info(\"Making data stationary\")\n",
    "    \n",
    "    # Apply log transformation\n",
    "    data[f'{target_column}_log'] = np.log(data[target_column])\n",
    "    \n",
    "    # Apply differencing\n",
    "    data[f'{target_column}_stationary'] = data[f'{target_column}_log'].diff().dropna()\n",
    "\n",
    "    # Test stationarity\n",
    "    test_stationarity(data[f'{target_column}_stationary'].dropna())\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Assuming preprocessed_data is already loaded from previous steps\n",
    "    target_column = 'Close'  # Define which column to make stationary\n",
    "    preprocessed_data.dropna(inplace=True)  # Drop NaN values for stationarity testing\n",
    "    stationary_data = make_stationary(preprocessed_data, target_column)\n",
    "    \n",
    "    logging.info(\"Stationarity transformation complete\")\n",
    "    return stationary_data\n",
    "\n",
    "# Run main to perform stationarity transformation\n",
    "stationary_data = main()\n",
    "\n",
    "\n",
    "# Print the stationary data\n",
    "#print(stationary_data)\n",
    "\n",
    "# Displaying the last 5 rows with a title\n",
    "display(HTML('<h2>Last 5 Rows of the DataFrame</h2>'))\n",
    "display(HTML(stationary_data.head(5).to_html()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca1ac02-43ef-43a9-a1fe-f76ea4556dc7",
   "metadata": {},
   "source": [
    "# ARIMA model Implementation - Univariate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ea932-5e12-477a-ad76-cd411147efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objs as go\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname=s - %(message=s')\n",
    "\n",
    "# Function to split data into training and testing datasets\n",
    "def split_data(data: pd.DataFrame, start_train: str, end_train: str, start_test: str, end_test: str) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"Splits the data into training and testing datasets.\"\"\"\n",
    "    train_data = data[start_train:end_train]\n",
    "    test_data = data[start_test:end_test]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Function to implement SARIMAX model and return fitted model\n",
    "def train_sarimax(train_data: pd.Series, p: int, d: int, q: int, seasonal_order=(0, 0, 0, 0)):\n",
    "    \"\"\"Trains the SARIMAX model and returns the fitted model.\"\"\"\n",
    "    model = SARIMAX(train_data, order=(p, d, q), seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit()\n",
    "    return model_fit\n",
    "\n",
    "# Function to visualize the train/test/forecast results with Plotly\n",
    "def plot_train_test_forecast(train_data: pd.Series, test_data: pd.Series, forecast: pd.Series) -> None:\n",
    "    \"\"\"Visualizes the train, test, and forecast data using Plotly.\"\"\"\n",
    "    fig = make_subplots(rows=1, cols=1)\n",
    "    \n",
    "    # Add traces for training, testing, and forecasted data\n",
    "    fig.add_trace(go.Scatter(x=train_data.index, y=train_data, mode='lines', name='Train Data'))\n",
    "    fig.add_trace(go.Scatter(x=test_data.index, y=test_data, mode='lines', name='Test Data'))\n",
    "    fig.add_trace(go.Scatter(x=forecast.index, y=forecast, mode='lines', name='Forecast'))\n",
    "    \n",
    "    # Set plot layout\n",
    "    fig.update_layout(\n",
    "        title='Train, Test and Forecast Data',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Stock Price',\n",
    "        hovermode='x',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Function to plot ACF and PACF with Plotly\n",
    "def plot_acf_pacf(data: pd.Series) -> None:\n",
    "    \"\"\"Plots the ACF and PACF using Plotly.\"\"\"\n",
    "    acf_values = acf(data, nlags=40)\n",
    "    pacf_values = pacf(data, nlags=40)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=('ACF', 'PACF'))\n",
    "\n",
    "    fig.add_trace(go.Bar(x=np.arange(len(acf_values)), y=acf_values, name='ACF'), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(x=np.arange(len(pacf_values)), y=pacf_values, name='PACF'), row=1, col=2)\n",
    "\n",
    "    fig.update_layout(title_text='ACF and PACF Plots', showlegend=False)\n",
    "    fig.show()\n",
    "\n",
    "# Main function for SARIMAX implementation and forecasting\n",
    "def main():\n",
    "    # Assuming `stationary_data` is already loaded from previous steps\n",
    "    target_column = 'Close_stationary'\n",
    "    train_start, train_end = '01-04-2019', '31-03-2023'\n",
    "    test_start, test_end = '01-04-2023', '31-03-2024'\n",
    "    forecast_start, forecast_end = '01-04-2024', '31-03-2025'\n",
    "\n",
    "    train_data, test_data = split_data(stationary_data[target_column].dropna(), train_start, train_end, test_start, test_end)\n",
    "    \n",
    "    # Plot ACF and PACF plots for SARIMAX parameter selection\n",
    "    plot_acf_pacf(train_data)\n",
    "    \n",
    "    # Train SARIMAX model with assumed p, d, q values and seasonal_order (these should be fine-tuned)\n",
    "    p, d, q, seasonal_order = 1, 1, 1, (1, 1, 1, 12)\n",
    "    model_fit = train_sarimax(train_data, p, d, q, seasonal_order)\n",
    "    \n",
    "    # Forecast for the next 12 months\n",
    "    forecast_steps = pd.date_range(start=forecast_start, end=forecast_end, freq='M').size\n",
    "    forecast = model_fit.forecast(steps=forecast_steps)\n",
    "    \n",
    "    # Set the forecast index to the expected date range\n",
    "    forecast.index = pd.date_range(start=forecast_start, end=forecast_end, freq='M')\n",
    "\n",
    "    # Visualization\n",
    "    plot_train_test_forecast(train_data, test_data, forecast)\n",
    "    \n",
    "    # Print SARIMAX results\n",
    "    print(model_fit.summary())\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0a036a-5b0f-4585-aa6c-5b72593d9f45",
   "metadata": {},
   "source": [
    "### Analyze Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e1a70b-cc5e-45a8-9adb-2d7c7230819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split data into training and testing datasets\n",
    "def split_data(data: pd.DataFrame, start_train: str, end_train: str, start_test: str, end_test: str) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"Splits the data into training and testing datasets.\"\"\"\n",
    "    train_data = data[start_train:end_train]\n",
    "    test_data = data[start_test:end_test]\n",
    "    return train_data, test_data\n",
    "\n",
    "# Function to implement SARIMAX model and return fitted model\n",
    "def train_sarimax(train_data: pd.Series, p: int, d: int, q: int, seasonal_order=(0, 0, 0, 0)):\n",
    "    \"\"\"Trains the SARIMAX model and returns the fitted model.\"\"\"\n",
    "    model = SARIMAX(train_data, order=(p, d, q), seasonal_order=seasonal_order)\n",
    "    model_fit = model.fit()\n",
    "    return model_fit\n",
    "\n",
    "# Function to analyze residuals\n",
    "def plot_residuals(residuals):\n",
    "    \"\"\"Analyzes residuals for autocorrelation, normality, and heteroscedasticity.\"\"\"\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # Residuals vs Time\n",
    "    sns.lineplot(x=residuals.index, y=residuals, ax=ax[0])\n",
    "    ax[0].set_title('Residuals vs Time')\n",
    "    \n",
    "    # Histogram\n",
    "    sns.histplot(residuals, kde=True, ax=ax[1])\n",
    "    ax[1].set_title('Histogram of Residuals')\n",
    "    \n",
    "    # Q-Q Plot\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax[2])\n",
    "    ax[2].set_title('Q-Q Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to calculate and print model accuracy metrics\n",
    "def calculate_accuracy(test_data, forecast):\n",
    "    \"\"\"Calculates and prints MSE, MAE, and RMSE.\"\"\"\n",
    "    mse = mean_squared_error(test_data, forecast)\n",
    "    mae = mean_absolute_error(test_data, forecast)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    logging.info(f'MSE: {mse:.3f}')\n",
    "    logging.info(f'MAE: {mae:.3f}')\n",
    "    logging.info(f'RMSE: {rmse:.3f}')\n",
    "    \n",
    "    print(f'MSE: {mse:.3f}')\n",
    "    print(f'MAE: {mae:.3f}')\n",
    "    print(f'RMSE: {rmse:.3f}')\n",
    "\n",
    "# Main function to train and analyze SARIMAX model\n",
    "def main():\n",
    "    # Assuming `stationary_data` is already loaded from previous steps\n",
    "    target_column = 'Close_stationary'\n",
    "    train_start, train_end = '01-04-2019', '31-03-2023'\n",
    "    test_start, test_end = '01-04-2023', '31-03-2024'\n",
    "    \n",
    "    train_data, test_data = split_data(stationary_data[target_column].dropna(), train_start, train_end, test_start, test_end)\n",
    "    \n",
    "    # Train SARIMAX model with assumed p, d, q values and seasonal_order (these should be fine-tuned)\n",
    "    p, d, q, seasonal_order = 1, 1, 1, (1, 1, 1, 12)\n",
    "    model_fit = train_sarimax(train_data, p, d, q, seasonal_order)\n",
    "    \n",
    "    # Forecast for the test period\n",
    "    forecast = model_fit.forecast(steps=len(test_data))\n",
    "    \n",
    "    # Plot residuals\n",
    "    plot_residuals(model_fit.resid)\n",
    "    \n",
    "    # Evaluate accuracy metrics\n",
    "    calculate_accuracy(test_data, forecast)\n",
    "    \n",
    "    # Print SARIMAX summary\n",
    "    print(model_fit.summary())\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f257a36a-ef18-4d56-ad6c-40c405b75c28",
   "metadata": {},
   "source": [
    "## ARIMA Model forecasting Analysis is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab3806-2392-4458-87c8-72c6a9f67cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40fc9874-2015-4704-abe9-ec1766b9430b",
   "metadata": {},
   "source": [
    "# Prophet model for forecasting  - Univariate model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb96cd1-874c-4aed-97ce-5043503ef910",
   "metadata": {},
   "source": [
    "### Preprocessing & Tuning Prophet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc52d79-a626-4831-8c2d-f0e320f1cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import logging\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "import plotly.graph_objs as go\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def download_stock_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Download stock data from Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading stock data for ticker: {ticker}\")\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        data.reset_index(inplace=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download stock data: {e}\")\n",
    "        raise\n",
    "\n",
    "def clean_data(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Clean stock data by filling missing values and removing duplicates.\"\"\"\n",
    "    logging.info(\"Cleaning data\")\n",
    "    data.fillna(method='ffill', inplace=True)\n",
    "    data.fillna(method='bfill', inplace=True)\n",
    "    data.drop_duplicates(inplace=True)\n",
    "    logging.info(\"Data cleaning complete\")\n",
    "    return data\n",
    "\n",
    "def preprocess_data_for_prophet(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for Prophet by renaming columns and formatting.\"\"\"\n",
    "    logging.info(\"Preparing data for Prophet model\")\n",
    "    prophet_data = data[['Date', 'Close']].rename(columns={'Date': 'ds', 'Close': 'y'})\n",
    "    logging.info(\"Data preparation complete\")\n",
    "    return prophet_data\n",
    "\n",
    "def tune_prophet_parameters() -> dict:\n",
    "    \"\"\"Tune Prophet model parameters for better accuracy.\"\"\"\n",
    "    return {\n",
    "        'seasonality_mode': 'multiplicative',\n",
    "        'yearly_seasonality': True,\n",
    "        'weekly_seasonality': True,\n",
    "        'daily_seasonality': False\n",
    "    }\n",
    "\n",
    "def prophet_cross_validation(data: pd.DataFrame, params: dict):\n",
    "    \"\"\"Perform cross-validation on the Prophet model.\"\"\"\n",
    "    logging.info(\"Performing cross-validation with Prophet\")\n",
    "    model = Prophet(**params)\n",
    "    model.fit(data)\n",
    "    df_cv = cross_validation(model, initial='730 days', period='180 days', horizon='365 days')\n",
    "    df_p = performance_metrics(df_cv)\n",
    "    logging.info(f\"Cross-validation metrics:\\n{df_p}\")\n",
    "    return df_cv, df_p\n",
    "\n",
    "def plot_cross_validation(df_cv: pd.DataFrame):\n",
    "    \"\"\"Plot the cross-validation forecast against actuals.\"\"\"\n",
    "    logging.info(\"Plotting cross-validation forecast\")\n",
    "    trace_actual = go.Scatter(x=df_cv['ds'], y=df_cv['y'], mode='lines', name='Actual')\n",
    "    trace_forecast = go.Scatter(x=df_cv['ds'], y=df_cv['yhat'], mode='lines', name='Forecast')\n",
    "    trace_upper = go.Scatter(x=df_cv['ds'], y=df_cv['yhat_upper'], mode='lines', name='Upper Confidence', line=dict(dash='dash'))\n",
    "    trace_lower = go.Scatter(x=df_cv['ds'], y=df_cv['yhat_lower'], mode='lines', name='Lower Confidence', line=dict(dash='dash'))\n",
    "    layout = go.Layout(title='Prophet Cross-Validation Forecast', xaxis={'title': 'Date'}, yaxis={'title': 'Stock Price'})\n",
    "    fig = go.Figure(data=[trace_actual, trace_forecast, trace_upper, trace_lower], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def main():\n",
    "    # Define parameters\n",
    "    ticker_symbol = 'TATAELXSI.NS'\n",
    "    start_date = '2019-04-01'\n",
    "    end_date = '2024-03-31'\n",
    "\n",
    "    # Download and preprocess data\n",
    "    stock_data = download_stock_data(ticker_symbol, start_date, end_date)\n",
    "    stock_data = clean_data(stock_data)\n",
    "    prophet_data = preprocess_data_for_prophet(stock_data)\n",
    "\n",
    "    # Perform cross-validation with Prophet\n",
    "    params = tune_prophet_parameters()\n",
    "    df_cv, _ = prophet_cross_validation(prophet_data, params)\n",
    "\n",
    "    # Plot cross-validation results\n",
    "    plot_cross_validation(df_cv)\n",
    "\n",
    "    # Display data summary in table format\n",
    "    logging.info(\"Displaying summary of the stock data:\")\n",
    "    summary = stock_data.describe()\n",
    "    logging.info(\"\\n\" + summary.to_string())  # Log summary as text\n",
    "    display(summary)  # Display summary in table format using pandas\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7115b4-7902-4076-a053-27fee25ef971",
   "metadata": {},
   "source": [
    "## Prophet Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf49c9-ec5c-4d0e-aa59-94eb455984d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_remove_outliers(data: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    \"\"\"Detect and remove outliers in the specified column using the IQR method.\"\"\"\n",
    "    logging.info(\"Detecting and removing outliers\")\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    mask = (data[column] >= Q1 - 1.5 * IQR) & (data[column] <= Q3 + 1.5 * IQR)\n",
    "    filtered_data = data[mask]\n",
    "    logging.info(\"Outlier removal complete\")\n",
    "    return filtered_data\n",
    "\n",
    "def preprocess_data_for_prophet(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Prepare data for Prophet by renaming columns and formatting.\"\"\"\n",
    "    logging.info(\"Preparing data for Prophet model\")\n",
    "    prophet_data = data[['Close']].reset_index()\n",
    "    prophet_data.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
    "    logging.info(\"Data preparation complete\")\n",
    "    return prophet_data\n",
    "\n",
    "def forecast_prophet(data: pd.DataFrame, periods: int) -> pd.DataFrame:\n",
    "    \"\"\"Forecast future stock prices using the Prophet model.\"\"\"\n",
    "    logging.info(\"Forecasting with Prophet\")\n",
    "    model = Prophet()\n",
    "    model.fit(data)\n",
    "    future = model.make_future_dataframe(periods=periods, freq='M')\n",
    "    forecast = model.predict(future)\n",
    "    return forecast\n",
    "\n",
    "def plot_forecast(actual_data: pd.DataFrame, forecast_data: pd.DataFrame) -> None:\n",
    "    \"\"\"Plot actual and forecasted stock prices.\"\"\"\n",
    "    trace1 = go.Scatter(x=actual_data['ds'], y=actual_data['y'], mode='lines', name='Actual')\n",
    "    trace2 = go.Scatter(x=forecast_data['ds'], y=forecast_data['yhat'], mode='lines', name='Forecast')\n",
    "    layout = go.Layout(title='Prophet Model Forecast', xaxis={'title': 'Date'}, yaxis={'title': 'Stock Price'})\n",
    "    fig = go.Figure(data=[trace1, trace2], layout=layout)\n",
    "    fig.show()\n",
    "\n",
    "def main():\n",
    "    # Define parameters\n",
    "    ticker_symbol = 'TATAELXSI.NS'\n",
    "    start_date = '2019-04-01'\n",
    "    end_date = '2024-03-31'\n",
    "    forecast_periods = 12\n",
    "\n",
    "    # Download and preprocess data\n",
    "    stock_data = download_stock_data(ticker_symbol, start_date, end_date)\n",
    "    stock_data = clean_data(stock_data)\n",
    "    stock_data = detect_and_remove_outliers(stock_data, 'Close')\n",
    "    prophet_data = preprocess_data_for_prophet(stock_data)\n",
    "\n",
    "    # Forecast with Prophet\n",
    "    forecast = forecast_prophet(prophet_data, forecast_periods)\n",
    "\n",
    "    # Display the forecast\n",
    "    display(HTML('<h2>First 5 Rows of the Forecast</h2>'))\n",
    "    display(HTML(forecast.head(5).to_html()))\n",
    "\n",
    "    # Plot the forecast\n",
    "    plot_forecast(prophet_data, forecast)\n",
    "\n",
    "    # Provide summary of the forecast\n",
    "    forecast_summary = forecast[['ds', 'yhat']].tail(forecast_periods).describe()\n",
    "    logging.info(\"Summary of the 12-month forecasted stock prices:\")\n",
    "    display(HTML('<h2>12-Month Forecast Summary</h2>'))\n",
    "    display(HTML(forecast_summary.to_html()))\n",
    "    print(forecast_summary)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd8e50-1924-4e4c-b591-24d4e9d9c661",
   "metadata": {},
   "source": [
    "## Prophet Model forecasting stock analysis ---- Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1fd15b-2e1e-4fe2-a7e2-f735e1b40381",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c7276d7-8d59-4f14-81cd-f9147bbf7fb7",
   "metadata": {},
   "source": [
    "# XGBoost Model - Multivariate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc647955-effa-46d6-98d1-b8bac6551d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime=s - %(message)s')\n",
    "\n",
    "def download_stock_data(ticker: str, start_date: str, end_date: str) -> pd.DataFrame:\n",
    "    \"\"\"Downloads stock data from Yahoo Finance.\"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Downloading stock data for ticker: {ticker}\")\n",
    "        data = yf.download(ticker, start=start_date, end=end_date)\n",
    "        data.reset_index(inplace=True)\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        data.set_index('Date', inplace=True)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download stock data: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Preprocesses the stock data for exploratory data analysis and model preparation.\"\"\"\n",
    "    # Handling missing values\n",
    "    missing_summary = df.isnull().sum()\n",
    "    logging.info(f\"Missing Values per Column:\\n{missing_summary}\")\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Removing duplicates\n",
    "    duplicates_count = df.duplicated().sum()\n",
    "    logging.info(f\"Number of Duplicate Rows: {duplicates_count}\")\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Adds engineered features to the stock data for analysis.\"\"\"\n",
    "    # Lag features\n",
    "    df['Lag1'] = df['Close'].shift(1)\n",
    "    df['Lag5'] = df['Close'].shift(5)\n",
    "    df['Lag10'] = df['Close'].shift(10)\n",
    "\n",
    "    # Moving averages\n",
    "    df['MA5'] = df['Close'].rolling(window=5).mean()\n",
    "    df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "\n",
    "    # Volatility\n",
    "    df['Volatility'] = df['Close'].rolling(window=5).std()\n",
    "\n",
    "    # Remove NaN values resulting from rolling windows\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_eda_report(df: pd.DataFrame):\n",
    "    \"\"\"Generates an EDA report with a heatmap for the feature-engineered data.\"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.show()\n",
    "\n",
    "def split_data(df: pd.DataFrame, test_size: float = 0.2) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Splits the data into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The feature-engineered stock data.\n",
    "        test_size (float): Fraction of the data to be used for testing.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]: X_train, X_test, y_train, y_test.\n",
    "    \"\"\"\n",
    "    X = df.drop(['Close'], axis=1)\n",
    "    y = df['Close']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def train_xgboost_model(X_train, y_train):\n",
    "    \"\"\"Trains an XGBoost model using grid search for hyperparameter tuning.\"\"\"\n",
    "    xg_reg = xgb.XGBRegressor(objective='reg:squarederror', seed=42)\n",
    "    \n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error', verbose=1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    logging.info(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def forecast_stock_prices(model, X_test, steps=12):\n",
    "    \"\"\"Forecasts stock prices for the next 12 months using the trained model.\"\"\"\n",
    "    future_predictions = []\n",
    "    last_valid_index = X_test.shape[0] - 1\n",
    "    current_input = X_test.iloc[last_valid_index, :].values.reshape(1, -1)\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        pred = model.predict(current_input)[0]\n",
    "        future_predictions.append(pred)\n",
    "        \n",
    "        current_input = np.roll(current_input, -1)\n",
    "        current_input[0, -1] = pred\n",
    "\n",
    "    return future_predictions\n",
    "\n",
    "def plot_forecast(y_test, predictions):\n",
    "    \"\"\"Plots the actual and forecasted stock prices using Plotly.\"\"\"\n",
    "    forecast_dates = pd.date_range(start=y_test.index[-1] + pd.DateOffset(1), periods=len(predictions), freq='M')\n",
    "    forecast_series = pd.Series(predictions, index=forecast_dates)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=y_test.index, y=y_test, mode='lines', name='Actual'))\n",
    "    fig.add_trace(go.Scatter(x=forecast_series.index, y=forecast_series, mode='lines', name='Forecast'))\n",
    "    fig.update_layout(title='Actual vs Forecasted Stock Price',\n",
    "                      xaxis_title='Date', yaxis_title='Stock Price')\n",
    "    fig.show()\n",
    "\n",
    "def summarize_forecast(predictions):\n",
    "    \"\"\"Summarizes forecast results by calculating descriptive statistics.\"\"\"\n",
    "    forecast_series = pd.Series(predictions)\n",
    "    summary_stats = forecast_series.describe()\n",
    "    logging.info(f\"Forecast Summary:\\n{summary_stats}\")\n",
    "\n",
    "def main():\n",
    "    ticker_symbol = 'TATAELXSI.NS'\n",
    "    start_date = '2019-04-01'\n",
    "    end_date = '2024-03-31'\n",
    "    \n",
    "    stock_data = download_stock_data(ticker_symbol, start_date, end_date)\n",
    "    preprocessed_data = preprocess_data(stock_data)\n",
    "    \n",
    "    # Feature Engineering Step\n",
    "    engineered_data = feature_engineering(preprocessed_data)\n",
    "\n",
    "    # Generate EDA Report\n",
    "    generate_eda_report(engineered_data)\n",
    "\n",
    "    # Train-Test Split\n",
    "    X_train, X_test, y_train, y_test = split_data(engineered_data, test_size=0.2)\n",
    "    logging.info(f\"Training set size: {X_train.shape[0]}, Testing set size: {X_test.shape[0]}\")\n",
    "\n",
    "    # Train XGBoost Model\n",
    "    model = train_xgboost_model(X_train, y_train)\n",
    "\n",
    "    # Forecast Stock Prices for the Next 12 Months\n",
    "    predictions = forecast_stock_prices(model, X_test, steps=12)\n",
    "    \n",
    "    # Summarize and Show Forecast Graph\n",
    "    summarize_forecast(predictions)\n",
    "    plot_forecast(y_test, predictions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263af6a0-e856-4f74-a0fd-952e28794fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987012f8-9d4b-43fc-9084-168f85ae49f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
